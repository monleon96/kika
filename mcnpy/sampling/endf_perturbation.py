"""
ENDF file perturbation module for angular distribution covariance data.

This module provides functionality to perturb ENDF MF4 angular distribution data
using MF34 covariance matrices. It is designed to be scalable and maintainable
for future extension to other MF sections.
"""
from typing import List, Union, Optional, Dict, Tuple, Any
from multiprocessing import Pool
from datetime import datetime
import os
import numpy as np
import pandas as pd
import shutil
import tempfile

from mcnpy.sampling.generators import generate_endf_samples
from mcnpy.cov.mf34_covmat import MF34CovMat
from mcnpy.endf.parsers.parse_endf import parse_endf_file
from mcnpy.endf.writers.endf_writer import ENDFWriter
from mcnpy.endf.classes.mf4.polynomial import MF4MTLegendre
from mcnpy.endf.classes.mf4.mixed import MF4MTMixed
from mcnpy.endf.classes.mf import MF
from mcnpy._utils import zaid_to_symbol, temperature_to_suffix
from mcnpy.njoy.run_njoy import run_njoy
from mcnpy.endf.read_endf import read_endf
from mcnpy.ace.xsdir import create_xsdir_files_for_ace


# Reuse the DualLogger from ace_perturbation
from mcnpy.sampling.ace_perturbation import DualLogger

# Import NJOY runner for ACE generation
from mcnpy.njoy.run_njoy import run_njoy

# Global logger instance
_logger = None

def _get_logger():
    """Get the global logger instance."""
    return _logger


def _process_sample(
    endf_file: str,
    sample: np.ndarray,
    sample_index: int,
    energy_grids: Dict[Tuple[int, int, int], List[float]],  # (isotope, mt, l) -> energy_grid
    param_mapping: List[Tuple[int, int, int, int]],  # List of (isotope, mt, l, energy_bin) tuples
    output_dir: str,
    dry_run: bool = False,
    generate_ace: bool = False,
    njoy_exe: Optional[str] = None,
    temperatures: Optional[List[float]] = None,
    library_name: Optional[str] = None,
    njoy_version: str = "NJOY 2016.78",
    xsdir_file: Optional[str] = None,
):
    """
    Process a single perturbation sample for ENDF files.
    
    Parameters
    ----------
    endf_file : str
        Path to the original ENDF file
    sample : np.ndarray
        Perturbation factors for this sample
    sample_index : int
        Index of this sample (0-based)
    energy_grids : Dict[Tuple[int, int, int], List[float]]
        Energy grids for each (isotope, mt, l) combination
    param_mapping : List[Tuple[int, int, int]]
        Mapping of sample indices to (isotope, mt, l) parameters
    output_dir : str
        Output directory for results
    dry_run : bool
        If True, only generate factors without creating ENDF files
    generate_ace : bool
        If True, generate ACE files using NJOY for each perturbed ENDF
    njoy_exe : Optional[str]
        Path to NJOY executable
    temperatures : Optional[Union[float, List[float]]]
        Temperature(s) for ACE generation (can be single float or list)
    library_name : Optional[str]
        Nuclear data library name
    njoy_version : str
        NJOY version string
    """
    if dry_run:
        # Parse the ENDF file to get ZAID for proper directory structure
        endf = parse_endf_file(endf_file)
        base = os.path.splitext(os.path.basename(endf_file))[0]
        sample_str = f"{sample_index+1:04d}"
        sample_dir = os.path.join(output_dir, "endf", str(endf.zaid or "unknown"), sample_str)
        os.makedirs(sample_dir, exist_ok=True)
        
        _write_sample_summary(
            sample=sample,
            sample_index=sample_index,
            energy_grids=energy_grids,
            param_mapping=param_mapping,
            sample_dir=sample_dir,
            base=base,
        )
        return
    
    # Parse the ENDF file
    endf = parse_endf_file(endf_file)
    
    # Apply perturbations to MF4 data
    perturbed_params = apply_perturbation_factors_to_endf(
        endf, sample, sample_index, energy_grids, param_mapping, verbose=False
    )
    
    # Write perturbed ENDF file
    base, ext = os.path.splitext(os.path.basename(endf_file))
    sample_str = f"{sample_index+1:04d}"
    sample_dir = os.path.join(output_dir, "endf", str(endf.zaid or "unknown"), sample_str)
    os.makedirs(sample_dir, exist_ok=True)
    out_endf = os.path.join(sample_dir, f"{base}_{sample_str}{ext}")
    
    # Use ENDF writer to save the modified file
    writer = ENDFWriter(endf_file)
    if 4 in endf.files:
        success = writer.replace_mf_section(endf.files[4], out_endf)
        if not success:
            if _get_logger():
                _get_logger().error(f"Failed to write perturbed ENDF file: {out_endf}")
            return
    
    # Write sample summary
    _write_sample_summary(
        sample=sample,
        sample_index=sample_index,
        energy_grids=energy_grids,
        param_mapping=param_mapping,
        sample_dir=sample_dir,
        base=base,
    )
    
    # Generate ACE files using NJOY if requested
    if generate_ace and not dry_run:
        njoy_result = _process_njoy_for_sample(
            out_endf=out_endf,
            sample_index=sample_index,
            njoy_exe=njoy_exe,
            temperatures=temperatures,
            library_name=library_name,
            njoy_version=njoy_version,
            output_dir=output_dir,
            xsdir_file=xsdir_file,
        )
        return njoy_result
    
    return {"success": True, "temperatures_processed": [], "errors": [], "warnings": []}


def _process_njoy_for_sample(
    out_endf: str,
    sample_index: int,
    njoy_exe: str,
    temperatures: List[float],
    library_name: str,
    njoy_version: str,
    output_dir: str,
    xsdir_file: Optional[str] = None,
):
    """
    Process a perturbed ENDF file through NJOY to generate ACE files.
    
    Parameters
    ----------
    out_endf : str
        Path to the perturbed ENDF file
    sample_index : int
        Sample index (0-based)
    njoy_exe : str
        Path to NJOY executable
    temperatures : List[float]
        List of temperatures for ACE generation
    library_name : str
        Nuclear data library name
    njoy_version : str
        NJOY version string
    output_dir : str
        Base output directory
        
    Returns
    -------
    Dict[str, Any]
        Dictionary with success status and any error messages
    """
    
    logger = _get_logger()
    sample_str = f"{sample_index+1:04d}"
    
    # Parse ENDF to get ZAID for directory organization
    try:
        endf_data = read_endf(out_endf)
        zaid = endf_data.zaid or "unknown"
    except Exception as e:
        if logger:
            logger.error(f"[NJOY] Sample {sample_str}: Failed to parse ENDF for ZAID - {e}")
        return {"success": False, "error": f"Failed to parse ENDF for ZAID: {e}"}
    
    results = {"success": True, "temperatures_processed": [], "errors": [], "warnings": []}
    
    for temp in temperatures:
        try:
            # Create custom directory structure: ace/temp/zaid/sample_num/ using exact temperature input
            temp_str = str(temp).rstrip('0').rstrip('.') if '.' in str(temp) else str(temp)
            ace_sample_dir = os.path.join(output_dir, "ace", temp_str, str(zaid), sample_str)
            njoy_sample_dir = os.path.join(output_dir, "njoy_files", temp_str, str(zaid), sample_str)
            # Create directories
            os.makedirs(ace_sample_dir, exist_ok=True)
            os.makedirs(njoy_sample_dir, exist_ok=True)
            
            # Run NJOY with a temporary directory (to avoid library subdirectories)
            with tempfile.TemporaryDirectory(prefix="njoy_temp_") as temp_dir:
                result = run_njoy(
                    njoy_exe=njoy_exe,
                    endf_path=out_endf,
                    temperature=temp,
                    library_name=library_name,
                    output_dir=temp_dir,  # Use temporary directory
                    njoy_version=njoy_version,
                    additional_suffix=sample_str,
                )
                
                if result["returncode"] == 0:
                    results["temperatures_processed"].append(temp)
                    
                    # Move ACE file to our custom structure
                    if result.get("ace_file") and os.path.exists(result["ace_file"]):
                        ace_filename = os.path.basename(result["ace_file"])
                        dest_ace = os.path.join(ace_sample_dir, ace_filename)
                        shutil.move(result["ace_file"], dest_ace)
                        
                        # Create xsdir files for the generated ACE file
                        if xsdir_file is not None:
                            try:
                                # Parse ACE file to get header information for xsdir creation
                                from mcnpy.ace.parsers import read_ace
                                ace_data = read_ace(dest_ace)
                                hdr = ace_data.header
                                has_ptable = bool(getattr(ace_data.unresolved_resonance, 'has_data', False))
                                
                                # Determine the proper cross-section library extension
                                # For NJOY-generated files, calculate extension based on temperature
                                base_ace, ace_file_ext = os.path.splitext(os.path.basename(dest_ace))
                                
                                # Convert temperature from MeV to Kelvin and get proper suffix
                                from mcnpy._utils import MeV_to_kelvin
                                temp_K = MeV_to_kelvin(hdr.temperature)
                                xs_ext = temperature_to_suffix(temp_K) + "c"  # Add 'c' for continuous energy
                                
                                create_xsdir_files_for_ace(
                                    ace_file_path=dest_ace,
                                    zaid=hdr.zaid,
                                    awr=hdr.atomic_weight_ratio,
                                    xss_len=hdr.nxs_array[1],
                                    temperature_mev=hdr.temperature,
                                    sample_index=sample_index,
                                    output_dir=output_dir,
                                    master_xsdir_file=xsdir_file,
                                    has_ptable=has_ptable,
                                )
                                    
                            except Exception as xsdir_err:
                                warning_msg = f"Failed to create XSDIR files at {temp}K: {xsdir_err}"
                                results["warnings"].append(warning_msg)
                                if logger:
                                    logger.warning(f"[NJOY] Sample {sample_str} at {temp}K: {warning_msg}")
                    
                    # Move NJOY auxiliary files to our custom structure
                    aux_files = ["njoy_input", "njoy_output", "xsdir_file", "viewr_output"]
                    for aux_file in aux_files:
                        if result.get(aux_file) and os.path.exists(result[aux_file]):
                            aux_filename = os.path.basename(result[aux_file])
                            dest_aux = os.path.join(njoy_sample_dir, aux_filename)
                            try:
                                shutil.move(result[aux_file], dest_aux)
                            except Exception as move_err:
                                warning_msg = f"Could not move {aux_file} at {temp}K: {move_err}"
                                results["warnings"].append(warning_msg)
                                if logger:
                                    logger.warning(f"[NJOY] Sample {sample_str}: {warning_msg}")
                else:
                    error_msg = f"NJOY failed at {temp}K with return code {result['returncode']}"
                    results["errors"].append(error_msg)
                    results["success"] = False
                    if logger:
                        logger.error(f"[NJOY] Sample {sample_str}: {error_msg}")
                        
        except Exception as e:
            error_msg = f"Exception at {temp}K: {e}"
            results["errors"].append(error_msg)
            results["success"] = False
            if logger:
                logger.error(f"[NJOY] Sample {sample_str}: {error_msg}")
    
    return results


def _log_njoy_batch_results(njoy_results, file_key, file_index, temperatures, summary_data):
    """
    Log NJOY processing results in batch to reduce log verbosity.
    
    Parameters
    ----------
    njoy_results : List[Tuple[int, Dict]]
        List of (sample_index, result_dict) tuples
    file_key : str
        File identifier for summary tracking
    file_index : int
        1-based file index
    temperatures : List[float]
        List of temperatures processed
    summary_data : Dict
        Summary data dictionary to update
    """
    logger = _get_logger()
    if not logger:
        return
    
    total_samples = len(njoy_results)
    successful_samples = sum(1 for _, result in njoy_results if result.get("success", False))
    failed_samples = total_samples - successful_samples
    
    # Count warnings and errors across all samples
    all_warnings = []
    all_errors = []
    temp_success_count = {temp: 0 for temp in temperatures}
    
    for sample_idx, result in njoy_results:
        if result.get("warnings"):
            all_warnings.extend(result["warnings"])
        if result.get("errors"):
            all_errors.extend(result["errors"])
        
        # Count successful temperatures
        for temp in result.get("temperatures_processed", []):
            temp_success_count[temp] += 1
    
    # Update summary data
    if file_key in summary_data:
        summary_data[file_key]['ace_generation']['successful_samples'] = successful_samples
        summary_data[file_key]['ace_generation']['failed_samples'] = failed_samples
    
    # Log batch summary
    logger.info(f"[NJOY] File {file_index}: Batch processing complete - {successful_samples}/{total_samples} samples successful")
    
    # Log temperature-specific results
    for temp in temperatures:
        success_count = temp_success_count[temp]
        temp_str = str(temp).rstrip('0').rstrip('.') if '.' in str(temp) else str(temp)
        logger.info(f"[NJOY] File {file_index}: Temperature {temp_str} - {success_count}/{total_samples} samples successful")
    
    # Log warnings (if any) - aggregate similar warnings
    if all_warnings:
        warning_counts = {}
        for warning in all_warnings:
            # Extract the core warning message (remove temperature-specific parts)
            core_warning = warning.split(" at ")[0] if " at " in warning else warning
            warning_counts[core_warning] = warning_counts.get(core_warning, 0) + 1
        
        logger.warning(f"[NJOY] File {file_index}: {len(all_warnings)} warnings occurred:")
        for warning, count in warning_counts.items():
            if count > 1:
                logger.warning(f"[NJOY] File {file_index}:   {warning} (occurred {count} times)")
            else:
                logger.warning(f"[NJOY] File {file_index}:   {warning}")
    
    # Log errors (if any) - aggregate similar errors
    if all_errors:
        error_counts = {}
        for error in all_errors:
            # Extract the core error message (remove temperature-specific parts)
            core_error = error.split(" at ")[0] if " at " in error else error
            error_counts[core_error] = error_counts.get(core_error, 0) + 1
        
        logger.error(f"[NJOY] File {file_index}: {len(all_errors)} errors occurred:")
        for error, count in error_counts.items():
            if count > 1:
                logger.error(f"[NJOY] File {file_index}:   {error} (occurred {count} times)")
            else:
                logger.error(f"[NJOY] File {file_index}:   {error}")


def load_mf34_covariance(path: str) -> Optional[MF34CovMat]:
    """
    Load MF34 covariance matrix from ENDF file containing MF34 section.
    
    Parameters
    ----------
    path : str
        Path to the ENDF file containing MF34 covariance data
        
    Returns
    -------
    Optional[MF34CovMat]
        Loaded MF34CovMat object or None if loading failed
    """
    if not os.path.exists(path):
        if _get_logger():
            _get_logger().error(f"ENDF file not found: {path}")
        return None
    
    try:
        logger = _get_logger()
        if logger:
            logger.info(f"[ENDF] [MF34] Loading covariance from file: {path}")
        
        # Parse the ENDF file
        endf = parse_endf_file(path)
        
        # Get MF34 section
        mf34 = endf.get_file(34)
        if mf34 is None:
            if logger:
                logger.error(f"[ENDF] [MF34] No MF34 section found in file: {path}")
            return None
        
        # Convert all MT sections to MF34CovMat and combine them
        combined_mf34_cov = MF34CovMat()
        
        for mt_number, mt_data in mf34.sections.items():
            if hasattr(mt_data, 'to_ang_covmat'):
                if logger:
                    logger.info(f"[ENDF] [MF34] Converting MT{mt_number} to angular covariance matrix")
                
                mt_covmat = mt_data.to_ang_covmat()
                
                # Add all matrices from this MT to the combined object
                for i in range(mt_covmat.num_matrices):
                    combined_mf34_cov.add_matrix(
                        isotope_row=mt_covmat.isotope_rows[i],
                        reaction_row=mt_covmat.reaction_rows[i],
                        l_row=mt_covmat.l_rows[i],
                        isotope_col=mt_covmat.isotope_cols[i],
                        reaction_col=mt_covmat.reaction_cols[i],
                        l_col=mt_covmat.l_cols[i],
                        matrix=mt_covmat.matrices[i],
                        energy_grid=mt_covmat.energy_grids[i]
                    )
        
        if logger:
            logger.info(f"[ENDF] [MF34] Successfully loaded covariance with {combined_mf34_cov.num_matrices} matrices")
            logger.info(f"Available isotopes: {sorted(combined_mf34_cov.isotopes)}")
            logger.info(f"Available reactions: {sorted(combined_mf34_cov.reactions)}")
            logger.info(f"Available Legendre indices: {sorted(combined_mf34_cov.legendre_indices)}")
        
        return combined_mf34_cov
        
    except Exception as e:
        if _get_logger():
            _get_logger().error(f"Failed to load MF34 covariance: {e}")
        return None


def perturb_ENDF_files(
    endf_files: Union[str, List[str]],
    mt_list: List[int],
    legendre_coeffs: List[int],
    num_samples: int,
    mf34_cov_files: Optional[Union[str, List[str]]] = None,
    space: str = "linear",
    decomposition_method: str = "svd",
    sampling_method: str = "sobol",
    output_dir: str = '.',
    seed: Optional[int] = None,
    nprocs: int = 1,
    dry_run: bool = False,
    verbose: bool = True,
    generate_ace: bool = False,
    njoy_exe: Optional[str] = None,
    temperatures: Optional[Union[float, List[float]]] = None,
    library_name: Optional[str] = None,
    njoy_version: str = "NJOY 2016.78",
    xsdir_file: Optional[str] = None,
):
    """
    Perturb ENDF nuclear data files using MF34 angular covariance matrices.
    
    This function generates perturbed ENDF files by sampling perturbation factors
    from multivariate normal distributions derived from MF34 covariance matrices.
    Optionally, it can also generate ACE files for each perturbed ENDF using NJOY.
    
    Parameters
    ----------
    endf_files : Union[str, List[str]]
        Path(s) to ENDF file(s) to be perturbed
    mt_list : List[int]
        List of MT reaction numbers to perturb. Empty list means all available MTs
    legendre_coeffs : List[int]
        List of Legendre coefficient indices to perturb (e.g., [0, 1, 2])
    num_samples : int
        Number of perturbed ENDF files to generate
    mf34_cov_files : Optional[Union[str, List[str]]], default None
        Path(s) to MF34 covariance matrix file(s). If None, covariance data will be
        read from the ENDF files themselves (MF34 section). If provided, can be a 
        single file (used for all ENDF files) or one file per ENDF file.
    space : str, default "linear"
        Sampling space: "linear"/"lin" (factors = 1 + X) or "log" (factors = exp(Y))
    decomposition_method : str, default "svd"
        Matrix decomposition method: "svd", "cholesky", "eigen", or "pca"
    sampling_method : str, default "sobol"
        Sampling method: "sobol", "lhs", or "random"
    output_dir : str, default "."
        Output directory for perturbed files
    seed : Optional[int], default None
        Random seed for reproducible sampling
    nprocs : int, default 1
        Number of parallel processes for sample processing
    dry_run : bool, default False
        If True, only generate perturbation factors without creating ENDF files
    verbose : bool, default True
        Enable verbose logging output
    generate_ace : bool, default False
        If True, generate ACE files for each perturbed ENDF using NJOY
    njoy_exe : Optional[str], default None
        Path to NJOY executable. Required if generate_ace is True
    temperatures : Optional[Union[float, List[float]]], default None
        Temperature(s) (in Kelvin) for ACE generation. Can be a single float or list of floats.
        Required if generate_ace is True.
    library_name : Optional[str], default None
        Nuclear data library name (e.g., 'endfb81', 'jeff40'). Required if generate_ace is True
    njoy_version : str, default "NJOY 2016.78"
        NJOY version string for metadata and titles
    xsdir_file : Optional[str], default None
        Path to master XSDIR file to be modified for each generated ACE file.
        Only used when generate_ace=True.
        
    Notes
    -----
    This function does not apply any autofix to the covariance matrices,
    using them exactly as provided in the MF34 data.
    
    When generate_ace=True, the output directory structure will include:
    - output_dir/endf/zaid/sample_num/ : perturbed ENDF files
    - output_dir/ace/temp/zaid/sample_num/ : ACE files organized by temperature (temp uses exact input format)
    - output_dir/njoy_files/temp/zaid/ : NJOY auxiliary files
    - output_dir/xsdir/ : modified xsdir files (if xsdir_file provided)
    - output_dir/*.log and *.parquet : log and master perturbation files
    """
    global _logger
    
    # Normalize space parameter: support both 'lin' and 'linear'
    if space.lower() == 'lin':
        space = 'linear'
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Setup logging
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(output_dir, f'endf_perturbation_{timestamp}.log')
    _logger = DualLogger(log_file)
    
    # Console: Basic start message
    print(f"[INFO] Starting ENDF perturbation job")
    print(f"[INFO] Log file: {log_file}")
    print(f"[INFO] Output directory: {os.path.abspath(output_dir)}")
    if generate_ace:
        print(f"[INFO] ACE generation enabled")
    
    # Validate NJOY parameters if ACE generation is requested
    if generate_ace:
        if njoy_exe is None:
            raise ValueError("njoy_exe must be provided when generate_ace=True")
        if temperatures is None:
            raise ValueError("temperatures must be provided when generate_ace=True")
        
        # Convert temperature to list if it's a single float
        if isinstance(temperatures, (int, float)):
            temperatures = [float(temperatures)]
        elif isinstance(temperatures, list):
            if len(temperatures) == 0:
                raise ValueError("temperatures list cannot be empty when generate_ace=True")
            temperatures = [float(t) for t in temperatures]
        else:
            raise ValueError("temperatures must be a float or list of floats")
            
        if library_name is None:
            raise ValueError("library_name must be provided when generate_ace=True")
        if not os.path.exists(njoy_exe):
            raise FileNotFoundError(f"NJOY executable not found: {njoy_exe}")
    
    # Print run parameters as metadata TO LOG FILE
    separator = "=" * 80
    _logger.info(f"\n{separator}")
    _logger.info(f"[ENDF] [PARAMETERS] Run Configuration")
    _logger.info(f"{separator}")
    
    # Format and print input files
    if isinstance(endf_files, str):
        endf_files = [endf_files]
        _logger.info(f"ENDF files: {endf_files[0]}")
    else:
        _logger.info(f"ENDF files ({len(endf_files)}):")
        for i, f in enumerate(endf_files):
            _logger.info(f"  [{i+1}] {f}")
    
    # Handle MF34 covariance files parameter
    if mf34_cov_files is None:
        _logger.info(f"MF34 covariance files: None (will read from ENDF files)")
        # Use ENDF files themselves as covariance sources
        mf34_cov_files = endf_files[:]
    elif isinstance(mf34_cov_files, str):
        mf34_cov_files = [mf34_cov_files]
        _logger.info(f"MF34 covariance files: {mf34_cov_files[0]}")
    else:
        _logger.info(f"MF34 covariance files ({len(mf34_cov_files)}):")
        for i, f in enumerate(mf34_cov_files):
            _logger.info(f"  [{i+1}] {f}")
    
    _logger.info(f"MT reactions: {mt_list}")
    _logger.info(f"Legendre coefficients: {legendre_coeffs}")
    _logger.info(f"Number of samples: {num_samples}")
    _logger.info(f"Sampling space: {space}")
    _logger.info(f"Decomposition method: {decomposition_method}")
    _logger.info(f"Sampling method: {sampling_method}")
    _logger.info(f"Random seed: {seed}")
    _logger.info(f"Parallel processes: {nprocs}")
    _logger.info(f"Dry run: {dry_run}")
    _logger.info(f"Generate ACE: {generate_ace}")
    if generate_ace:
        _logger.info(f"NJOY executable: {njoy_exe}")
        _logger.info(f"Temperatures: {temperatures}")
        _logger.info(f"Library name: {library_name}")
        _logger.info(f"NJOY version: {njoy_version}")
        _logger.info(f"XSDIR file: {xsdir_file if xsdir_file else 'None'}")
    _logger.info(f"{separator}")
    
    # Validate inputs
    if mf34_cov_files is not None and len(mf34_cov_files) != 1 and len(mf34_cov_files) != len(endf_files):
        raise ValueError("Number of MF34 covariance files must be 1 or equal to number of ENDF files")
    
    # Initialize summary tracking
    summary_data = {}
    failed_files_details = {}
    
    # Process each ENDF file
    all_factors = []
    all_param_mappings = []
    all_energy_grids = []
    processed_files = 0
    failed_files = 0
    
    for i, endf_file in enumerate(endf_files):
        try:
            _logger.info(f"\n[ENDF] Processing file {i+1}/{len(endf_files)}: {os.path.basename(endf_file)}")
            
            # Initialize summary data for this file
            file_key = os.path.basename(endf_file)
            summary_data[file_key] = {
                'file_path': endf_file,
                'file_index': i + 1,
                'perturbed_mts': [],
                'perturbed_l_coeffs': [],
                'num_samples': num_samples,
                'warnings': [],
                'ace_generation': {
                    'enabled': generate_ace,
                    'successful_samples': 0,
                    'failed_samples': 0,
                    'temperatures': temperatures if generate_ace else None,
                    'xsdir_created': False
                }
            }
            
            # Determine which covariance file to use
            cov_file = mf34_cov_files[0] if len(mf34_cov_files) == 1 else mf34_cov_files[i]
            
            # Load covariance matrix
            mf34_cov = load_mf34_covariance(cov_file)
            if mf34_cov is None:
                _logger.error(f"[ENDF] File {i+1}: Failed to load covariance matrix from {cov_file}")
                failed_files_details[file_key] = "Failed to load MF34 covariance matrix"
                failed_files += 1
                continue
            
            # Filter covariance data by requested MTs and Legendre coefficients
            filtered_cov = _filter_mf34_covariance(mf34_cov, mt_list, legendre_coeffs)
            
            if filtered_cov.num_matrices == 0:
                _logger.warning(f"[ENDF] File {i+1}: No covariance data found for requested MTs {mt_list} and L coefficients {legendre_coeffs}")
                failed_files_details[file_key] = f"No covariance data for requested MTs {mt_list} and L coefficients {legendre_coeffs}"
                failed_files += 1
                continue
            
            # Store which MTs and L coefficients will be perturbed
            summary_data[file_key]['perturbed_mts'] = list(filtered_cov.reactions)
            summary_data[file_key]['perturbed_l_coeffs'] = list(filtered_cov.legendre_indices)
            
            # Create parameter mapping and energy grids
            param_mapping, energy_grids = _create_parameter_mapping(filtered_cov)
            
            # Generate perturbation factors
            _logger.info(f"[ENDF] File {i+1}: Generating {num_samples} perturbation samples...")
            
            try:
                factors, _ = generate_endf_samples(
                    filtered_cov,
                    num_samples,
                    space=space,
                    decomposition_method=decomposition_method,
                    sampling_method=sampling_method,
                    seed=seed,
                    mt_numbers=mt_list,
                    verbose=verbose,
                )
                
                _logger.info(f"[ENDF] File {i+1}: Successfully generated perturbation factors: shape {factors.shape}")
                
            except Exception as e:
                _logger.error(f"[ENDF] File {i+1}: Failed to generate perturbation factors: {e}")
                failed_files_details[file_key] = f"Perturbation generation failed: {str(e)}"
                failed_files += 1
                continue
            
            # Store factors and mapping for master file
            all_factors.append(factors)
            all_param_mappings.append(param_mapping)
            all_energy_grids.append(energy_grids)
            
            # Process samples
            if not dry_run:
                _logger.info(f"[ENDF] File {i+1}: Applying perturbations to {num_samples} samples...")
            else:
                _logger.info(f"[ENDF] File {i+1}: Dry run: generating factor summaries for {num_samples} samples...")
            
            # Process samples with optional parallelization
            njoy_results = []  # Track NJOY results for batch logging
            
            if nprocs > 1 and num_samples > 1:
                _logger.info(f"[ENDF] File {i+1}: Using {nprocs} processes for parallel processing")
                
                try:
                    with Pool(processes=nprocs) as pool:
                        futures = []
                        for sample_idx in range(num_samples):
                            args = (
                                endf_file, factors[sample_idx], sample_idx, energy_grids, 
                                param_mapping, output_dir, dry_run, generate_ace, njoy_exe, 
                                temperatures, library_name, njoy_version, xsdir_file
                            )
                            future = pool.apply_async(_process_sample, args=args)
                            futures.append(future)
                        
                        # Wait for all processes to complete
                        pool.close()
                        pool.join()
                        
                        # Collect results and check for any exceptions
                        for j, future in enumerate(futures):
                            try:
                                result = future.get()  # This will raise any exception that occurred
                                if result and generate_ace and not dry_run:
                                    njoy_results.append((j, result))
                            except Exception as e:
                                _logger.error(f"[ENDF] File {i+1}: Sample {j+1:04d} processing failed: {e}")
                                
                except Exception as e:
                    _logger.error(f"[ENDF] File {i+1}: Parallel processing failed, falling back to serial: {e}")
                    # Fall back to serial processing
                    njoy_results = []
                    for sample_idx in range(num_samples):
                        try:
                            result = _process_sample(
                                endf_file=endf_file,
                                sample=factors[sample_idx],
                                sample_index=sample_idx,
                                energy_grids=energy_grids,
                                param_mapping=param_mapping,
                                output_dir=output_dir,
                                dry_run=dry_run,
                                generate_ace=generate_ace,
                                njoy_exe=njoy_exe,
                                temperatures=temperatures,
                                library_name=library_name,
                                njoy_version=njoy_version,
                                xsdir_file=xsdir_file,
                            )
                            if result and generate_ace and not dry_run:
                                njoy_results.append((sample_idx, result))
                        except Exception as sample_e:
                            _logger.error(f"[ENDF] File {i+1}: Sample {sample_idx+1:04d} processing failed: {sample_e}")
                            continue
            else:
                # Serial processing
                for sample_idx in range(num_samples):
                    try:
                        result = _process_sample(
                            endf_file=endf_file,
                            sample=factors[sample_idx],
                            sample_index=sample_idx,
                            energy_grids=energy_grids,
                            param_mapping=param_mapping,
                            output_dir=output_dir,
                            dry_run=dry_run,
                            generate_ace=generate_ace,
                            njoy_exe=njoy_exe,
                            temperatures=temperatures,
                            library_name=library_name,
                            njoy_version=njoy_version,
                            xsdir_file=xsdir_file,
                        )
                        if result and generate_ace and not dry_run:
                            njoy_results.append((sample_idx, result))
                    except Exception as e:
                        _logger.error(f"[ENDF] File {i+1}: Sample {sample_idx+1:04d} processing failed: {e}")
                        continue
            
            # Batch logging for NJOY results
            if generate_ace and not dry_run and njoy_results:
                _log_njoy_batch_results(njoy_results, file_key, i+1, temperatures, summary_data)
            
            processed_files += 1
            _logger.info(f"[ENDF] File {i+1}: Successfully processed all samples")
            
        except Exception as file_error:
            _logger.error(f"[ENDF] File {i+1}: Critical error during processing: {file_error}")
            failed_files_details[file_key] = f"Critical processing error: {str(file_error)}"
            failed_files += 1
            continue
    
    # Write master perturbation factors file
    master_file = _write_master_perturbation_file(
        all_factors, all_param_mappings, all_energy_grids, output_dir, timestamp, verbose
    )
    
    # =====================================================================
    #  Print final comprehensive summary for all files TO LOG FILE
    # =====================================================================
    separator = "=" * 80
    _logger.info(f"\n{separator}")
    _logger.info(f"[ENDF] [SUMMARY] Processing Results")
    _logger.info(f"{separator}")
    
    if not summary_data and not failed_files_details:
        _logger.info("  No ENDF files were processed.")
    else:
        # Report files that were successfully processed
        successfully_processed = {k: v for k, v in summary_data.items() if k not in failed_files_details}
        
        if successfully_processed:
            _logger.info("\n  SUCCESSFULLY PROCESSED ENDF FILES:")
            _logger.info(f"  {'-' * 60}")
            
            for file_key, data in successfully_processed.items():
                _logger.info(f"\n  File: {file_key}")
                _logger.info(f"  {'-' * 60}")
                
                # MTs and Legendre coefficients that were perturbed
                if data['perturbed_mts']:
                    _logger.info(f"  ► Perturbed MT numbers: {', '.join(map(str, sorted(data['perturbed_mts'])))}")
                if data['perturbed_l_coeffs']:
                    _logger.info(f"  ► Perturbed Legendre coefficients: {', '.join(map(str, sorted(data['perturbed_l_coeffs'])))}")
                
                _logger.info(f"  ► Number of samples generated: {data['num_samples']}")
                
                # ACE generation information
                ace_info = data['ace_generation']
                if ace_info['enabled']:
                    _logger.info(f"  ► ACE generation: ENABLED")
                    if ace_info['temperatures']:
                        temps_str = ', '.join(str(t).rstrip('0').rstrip('.') if '.' in str(t) else str(t) for t in ace_info['temperatures'])
                        _logger.info(f"    • Temperatures: {temps_str}")
                    
                    # Note: We would need to update the NJOY processing to track successful/failed ACE generations
                    # For now, just report if it was enabled
                    if xsdir_file:
                        _logger.info(f"    • XSDIR files: Generated from master file {os.path.basename(xsdir_file)}")
                else:
                    _logger.info(f"  ► ACE generation: DISABLED")
                
                # Warnings if any
                if data['warnings']:
                    _logger.info(f"  ► Warnings:")
                    for warning in data['warnings']:
                        _logger.info(f"    • {warning}")

        # Report files that failed processing
        if failed_files_details:
            _logger.info("\n  FAILED ENDF FILES:")
            _logger.info(f"  {'-' * 60}")
            
            for file_key, reason in failed_files_details.items():
                _logger.info(f"  File: {file_key}")
                _logger.info(f"    • Reason: {reason}")

    _logger.info(f"\n{separator}")
    
    # Console: Final summary
    processed_count = len(successfully_processed) if 'successfully_processed' in locals() else processed_files
    failed_count = len(failed_files_details)
    
    print(f"\n[INFO] ENDF perturbation job completed!")
    print(f"[INFO] Processed: {processed_count} file(s)")
    if failed_count > 0:
        print(f"[WARNING] Failed: {failed_count} file(s)")
    print(f"[INFO] Detailed log saved to: {log_file}")
    
    if master_file:
        print(f"[INFO] Master matrix file: {os.path.basename(master_file)}")
    
    if generate_ace:
        print(f"[INFO] ACE generation: {'ENABLED' if generate_ace else 'DISABLED'}")
        if generate_ace and xsdir_file:
            print(f"[INFO] XSDIR files generated in: xsdir/ subdirectory")


def apply_perturbation_factors_to_endf(
    endf, 
    sample: np.ndarray, 
    sample_index: int, 
    energy_grids: Dict[Tuple[int, int, int], List[float]], 
    param_mapping: List[Tuple[int, int, int, int]], 
    verbose: bool = True
):
    """
    Apply perturbation factors to ENDF MF4 angular distribution data.
    
    Parameters
    ----------
    endf : ENDF
        ENDF object with parsed data
    sample : np.ndarray
        Array of perturbation factors
    sample_index : int
        Index of this sample
    energy_grids : Dict[Tuple[int, int, int], List[float]]
        Energy grids for each parameter combination
    param_mapping : List[Tuple[int, int, int, int]]
        Mapping of sample indices to (isotope, mt, l, energy_bin) parameters
    verbose : bool
        Whether to log perturbation details
        
    Returns
    -------
    List[Tuple[int, int, int, int]]
        List of perturbed parameter combinations
    """
    if verbose and _get_logger():
        _get_logger().info(f"[ENDF] [SAMPLE] Applying perturbations to sample {sample_index + 1}")
    
    perturbed_params = []
    
    # Get MF4 data
    mf4 = endf.get_file(4)
    if mf4 is None:
        if _get_logger():
            _get_logger().warning("No MF4 data found in ENDF file")
        return perturbed_params
    
    # Apply perturbations to each MT section
    for mt_number, mt_data in mf4.sections.items():
        # Check for both MF4MTLegendre and MF4MTMixed (both have Legendre coefficients)
        if isinstance(mt_data, (MF4MTLegendre, MF4MTMixed)):
            # Apply perturbations to Legendre coefficients
            _apply_factors_to_mf4_legendre(
                mt_data, sample, param_mapping, energy_grids, verbose
            )
            # Add all perturbed parameters for this MT
            for isotope, mt, l_coeff, energy_bin in param_mapping:
                if mt == mt_number:
                    perturbed_params.append((isotope, mt, l_coeff, energy_bin))
    
    return perturbed_params


def _apply_factors_to_mf4_legendre(
    mt_data: Union[MF4MTLegendre, MF4MTMixed],
    factors: np.ndarray,
    param_mapping: List[Tuple[int, int, int, int]],
    energy_grids: Dict[Tuple[int, int, int], List[float]],
    verbose: bool = True
):
    """
    Apply perturbation factors to MF4 Legendre coefficient data with proper discontinuity handling.
    
    This function implements the ENDF-6 standard for representing discontinuities by duplicating
    energy points at bin boundaries to encode proper jumps in the angular distributions.
    
    Works with both MF4MTLegendre (LTT=1) and MF4MTMixed (LTT=3) types.
    
    Parameters
    ----------
    mt_data : Union[MF4MTLegendre, MF4MTMixed]
        MF4 MT section with Legendre coefficients
    factors : np.ndarray
        Perturbation factors
    param_mapping : List[Tuple[int, int, int, int]]
        Mapping of factor indices to parameters: (isotope, mt, l_coeff, energy_bin)
    energy_grids : Dict[Tuple[int, int, int], List[float]]
        Energy grids for each parameter combination
    verbose : bool
        Whether to log details
    """
    # Get the current coefficients
    current_coeffs = mt_data.legendre_coefficients
    energies = mt_data.legendre_energies
    
    if not current_coeffs or not energies:
        if verbose and _get_logger():
            _get_logger().warning(f"[ENDF] [MT{mt_data.number}] No Legendre coefficients found")
        return
    
    # Step 0: Make baseline copies before any scaling
    E0 = mt_data._energies[:]  # Original energy grid
    A0 = [c[:] for c in mt_data._legendre_coeffs]  # Original coefficients (deep copy)
    
    if verbose and _get_logger():
        _get_logger().debug(f"[ENDF] [MT{mt_data.number}] Baseline: {len(E0)} energy points, max coeffs per point: {max(len(c) for c in A0) if A0 else 0}")
    
    # Step 1: Gather boundaries from all energy grids for this MT
    boundaries = set()
    
    for factor_idx, (isotope, mt, l_coeff, energy_bin) in enumerate(param_mapping):
        if mt != mt_data.number:
            continue
            
        triplet = (isotope, mt, l_coeff)
        energy_grid = energy_grids.get(triplet, [])
        
        if len(energy_grid) < 2:
            continue
            
        # Add internal bin boundaries (not the first and last)
        for i in range(1, len(energy_grid) - 1):
            boundary = energy_grid[i]
            # Only include if within MF4 energy span
            if E0[0] <= boundary <= E0[-1]:
                boundaries.add(boundary)
    
    boundaries = sorted(list(boundaries))
    
    if verbose and _get_logger():
        _get_logger().debug(f"[ENDF] [MT{mt_data.number}] Found {len(boundaries)} bin boundaries: {boundaries}")
    
    # Step 2: Scale interior points (not at boundaries)
    applied_count = 0
    
    for factor_idx, (isotope, mt, l_coeff, energy_bin) in enumerate(param_mapping):
        if mt != mt_data.number:
            continue
        
        if factor_idx >= len(factors):
            continue
        
        factor = factors[factor_idx]
        
        # Get the energy grid for this parameter triplet
        triplet = (isotope, mt, l_coeff)
        energy_grid = energy_grids.get(triplet, [])
        
        if len(energy_grid) < 2:
            continue
        
        # Get energy bounds for this bin
        if energy_bin >= len(energy_grid) - 1:
            continue
            
        energy_low = energy_grid[energy_bin]
        energy_high = energy_grid[energy_bin + 1]
        
        # Apply factor to coefficients strictly inside this bin (not at boundaries)
        for energy_idx, energy in enumerate(mt_data._energies):
            if energy_low <= energy < energy_high and energy not in boundaries:
                # Check if this L coefficient exists at this energy
                coeff_index = l_coeff - 1  # Convert L=1,2,3... to 0-indexed
                if (coeff_index >= 0 and energy_idx < len(mt_data._legendre_coeffs) and 
                    coeff_index < len(mt_data._legendre_coeffs[energy_idx])):
                    
                    old_value = mt_data._legendre_coeffs[energy_idx][coeff_index]
                    mt_data._legendre_coeffs[energy_idx][coeff_index] *= factor
                    applied_count += 1
                    
                    if verbose and _get_logger():
                        _get_logger().debug(
                            f"[ENDF] [INTERIOR] MT{mt} L{l_coeff} at {energy:.3e} MeV: "
                            f"factor {factor:.6f}, {old_value:.3e} -> {mt_data._legendre_coeffs[energy_idx][coeff_index]:.3e}"
                        )
    
    # Step 3 & 4: Handle discontinuities at boundaries
    insertions_made = 0
    
    for boundary_energy in boundaries:
        if verbose and _get_logger():
            _get_logger().debug(f"[ENDF] [BOUNDARY] Processing boundary at {boundary_energy:.3e} MeV")
        
        # Interpolate baseline coefficients at this boundary
        baseline_coeffs = _interpolate_legendre_coefficients(boundary_energy, E0, A0)
        
        if baseline_coeffs is None:
            if verbose and _get_logger():
                _get_logger().warning(f"[ENDF] [BOUNDARY] Could not interpolate at {boundary_energy:.3e} MeV")
            continue
        
        # Find the factors for lower and upper bins at this boundary
        lower_factors = {}  # l_coeff -> factor for the bin below this boundary
        upper_factors = {}  # l_coeff -> factor for the bin above this boundary
        
        for factor_idx, (isotope, mt, l_coeff, energy_bin) in enumerate(param_mapping):
            if mt != mt_data.number:
                continue
                
            triplet = (isotope, mt, l_coeff)
            energy_grid = energy_grids.get(triplet, [])
            
            if len(energy_grid) < 2 or energy_bin >= len(energy_grid) - 1:
                continue
            
            factor = factors[factor_idx]
            energy_low = energy_grid[energy_bin]
            energy_high = energy_grid[energy_bin + 1]
            
            # Check which side of the boundary this bin is on
            if abs(energy_high - boundary_energy) < 1e-10:  # This bin ends at the boundary
                lower_factors[l_coeff] = factor  # This factor applies to the lower side
            if abs(energy_low - boundary_energy) < 1e-10:  # This bin starts at the boundary  
                upper_factors[l_coeff] = factor  # This factor applies to the upper side
        
        # If no factors found, skip this boundary
        if not lower_factors and not upper_factors:
            continue
        
        # Apply factors to create lower and upper coefficient vectors
        coeffs_minus = baseline_coeffs[:]
        coeffs_plus = baseline_coeffs[:]
        
        for l_coeff, factor in lower_factors.items():
            coeff_index = l_coeff - 1
            if 0 <= coeff_index < len(coeffs_minus):
                coeffs_minus[coeff_index] *= factor
        
        for l_coeff, factor in upper_factors.items():
            coeff_index = l_coeff - 1
            if 0 <= coeff_index < len(coeffs_plus):
                coeffs_plus[coeff_index] *= factor
        
        # Insert or replace the boundary points
        _insert_boundary_discontinuity(
            mt_data, boundary_energy, coeffs_minus, coeffs_plus, verbose
        )
        insertions_made += 1
    
    # Step 5: Update bookkeeping for ENDF output
    mt_data._ne = len(mt_data._energies)
    if mt_data._ne > 0:
        mt_data._interpolation = [(mt_data._ne, 2)]  # One region, linear-linear
        mt_data._nr = 1
    
    # For MF4MTMixed, also update Legendre-specific attributes
    if isinstance(mt_data, MF4MTMixed):
        mt_data._ne1 = mt_data._ne  # Number of Legendre energy points
        # Note: _ne2 and _nr_tab are for the tabulated part and should remain unchanged
    
    if verbose and _get_logger():
        _get_logger().info(
            f"[ENDF] [MT{mt_data.number}] Applied {applied_count} interior factors and "
            f"{insertions_made} boundary discontinuities. Final: {mt_data._ne} energy points"
        )


def _interpolate_legendre_coefficients(
    energy: float, 
    energy_grid: List[float], 
    coeff_grid: List[List[float]]
) -> Optional[List[float]]:
    """
    Interpolate Legendre coefficients at a given energy using linear-linear interpolation.
    
    Parameters
    ----------
    energy : float
        Energy at which to interpolate
    energy_grid : List[float]
        Original energy grid
    coeff_grid : List[List[float]]
        Coefficient arrays for each energy point
        
    Returns
    -------
    Optional[List[float]]
        Interpolated coefficients, or None if interpolation fails
    """
    if len(energy_grid) != len(coeff_grid) or len(energy_grid) < 2:
        return None
    
    # Find bracketing energies
    if energy <= energy_grid[0]:
        return coeff_grid[0][:]  # Return copy of first point
    if energy >= energy_grid[-1]:
        return coeff_grid[-1][:]  # Return copy of last point
    
    # Find the interval containing this energy
    for i in range(len(energy_grid) - 1):
        if energy_grid[i] <= energy <= energy_grid[i + 1]:
            e1, e2 = energy_grid[i], energy_grid[i + 1]
            c1, c2 = coeff_grid[i], coeff_grid[i + 1]
            
            # Linear interpolation factor
            if abs(e2 - e1) < 1e-15:  # Avoid division by zero
                return c1[:]
            
            t = (energy - e1) / (e2 - e1)
            
            # Interpolate each coefficient
            max_coeffs = max(len(c1), len(c2))
            result = []
            
            for j in range(max_coeffs):
                v1 = c1[j] if j < len(c1) else 0.0
                v2 = c2[j] if j < len(c2) else 0.0
                result.append(v1 + t * (v2 - v1))
            
            return result
    
    return None


def _insert_boundary_discontinuity(
    mt_data: Union[MF4MTLegendre, MF4MTMixed],
    boundary_energy: float,
    coeffs_minus: List[float],
    coeffs_plus: List[float],
    verbose: bool = True
):
    """
    Insert a discontinuity at a boundary energy by duplicating the energy point.
    
    Works with both MF4MTLegendre and MF4MTMixed types.
    
    Parameters
    ----------
    mt_data : Union[MF4MTLegendre, MF4MTMixed]
        MF4 section to modify
    boundary_energy : float
        Energy at which to insert discontinuity
    coeffs_minus : List[float]
        Coefficients for the "minus" side (lower bin)
    coeffs_plus : List[float]
        Coefficients for the "plus" side (upper bin)
    verbose : bool
        Whether to log details
    """
    # Find if this energy already exists
    existing_indices = [i for i, e in enumerate(mt_data._energies) if abs(e - boundary_energy) < 1e-10]
    
    if existing_indices:
        # Replace existing point with two consecutive points
        idx = existing_indices[0]
        
        if verbose and _get_logger():
            _get_logger().debug(f"[ENDF] [BOUNDARY] Replacing existing point at {boundary_energy:.3e} MeV")
        
        # Replace the existing point with minus side
        mt_data._energies[idx] = boundary_energy
        mt_data._legendre_coeffs[idx] = coeffs_minus[:]
        
        # Insert plus side right after
        mt_data._energies.insert(idx + 1, boundary_energy)
        mt_data._legendre_coeffs.insert(idx + 1, coeffs_plus[:])
        
    else:
        # Find insertion point to maintain sorted order
        insert_idx = 0
        for i, e in enumerate(mt_data._energies):
            if e < boundary_energy:
                insert_idx = i + 1
            else:
                break
        
        if verbose and _get_logger():
            _get_logger().debug(f"[ENDF] [BOUNDARY] Inserting new points at {boundary_energy:.3e} MeV at index {insert_idx}")
        
        # Insert minus side first
        mt_data._energies.insert(insert_idx, boundary_energy)
        mt_data._legendre_coeffs.insert(insert_idx, coeffs_minus[:])
        
        # Insert plus side right after
        mt_data._energies.insert(insert_idx + 1, boundary_energy)
        mt_data._legendre_coeffs.insert(insert_idx + 1, coeffs_plus[:])


def _find_energy_group(energy: float, energy_grid: List[float]) -> int:
    """
    Find the energy group index for a given energy.
    
    Parameters
    ----------
    energy : float
        Energy value
    energy_grid : List[float]
        Energy group boundaries
        
    Returns
    -------
    int
        Energy group index, or -1 if not found
    """
    for i in range(len(energy_grid) - 1):
        if energy_grid[i] <= energy < energy_grid[i + 1]:
            return i
    return -1


def _filter_mf34_covariance(
    mf34_cov: MF34CovMat, 
    mt_list: List[int], 
    legendre_coeffs: List[int]
) -> MF34CovMat:
    """
    Filter MF34 covariance data by MT reactions and Legendre coefficients.
    
    Parameters
    ----------
    mf34_cov : MF34CovMat
        Original MF34 covariance data
    mt_list : List[int]
        List of MT numbers to include (empty means all)
    legendre_coeffs : List[int]
        List of Legendre coefficient indices to include
        
    Returns
    -------
    MF34CovMat
        Filtered covariance data
    """
    if _get_logger():
        _get_logger().info(f"Filtering MF34 covariance data: MT={mt_list}, L={legendre_coeffs}")
        if not legendre_coeffs or legendre_coeffs == [-1]:
            _get_logger().info(f"Using all available Legendre coefficients: {sorted(mf34_cov.legendre_indices)}")
    
    filtered_cov = MF34CovMat()
    
    # If mt_list is empty, use all available MTs
    available_mts = mt_list if mt_list else list(mf34_cov.reactions)
    
    # If legendre_coeffs is empty or contains only -1, use all available L coefficients
    if not legendre_coeffs or legendre_coeffs == [-1]:
        available_ls = list(mf34_cov.legendre_indices)
        if _get_logger():
            _get_logger().info(f"Available L coefficients: {available_ls}")
    else:
        available_ls = legendre_coeffs
    
    for i in range(mf34_cov.num_matrices):
        mt_row = mf34_cov.reaction_rows[i]
        mt_col = mf34_cov.reaction_cols[i]
        l_row = mf34_cov.l_rows[i]
        l_col = mf34_cov.l_cols[i]
        
        # Check if this matrix should be included
        include_mt = (mt_row in available_mts and mt_col in available_mts)
        include_l = (l_row in available_ls and l_col in available_ls)
        
        if include_mt and include_l:
            filtered_cov.add_matrix(
                isotope_row=mf34_cov.isotope_rows[i],
                reaction_row=mt_row,
                l_row=l_row,
                isotope_col=mf34_cov.isotope_cols[i],
                reaction_col=mt_col,
                l_col=l_col,
                matrix=mf34_cov.matrices[i],
                energy_grid=mf34_cov.energy_grids[i]
            )
    
    if _get_logger():
        _get_logger().info(f"Filtered to {filtered_cov.num_matrices} matrices")
    
    return filtered_cov


def _create_parameter_mapping(mf34_cov: MF34CovMat) -> Tuple[List[Tuple[int, int, int, int]], Dict[Tuple[int, int, int], List[float]]]:
    """
    Create parameter mapping and energy grids from MF34 covariance data.
    Each parameter now includes energy bin index: (isotope, MT, l, energy_bin_index).
    
    The parameter mapping must match exactly how MF34CovMat.covariance_matrix builds
    the full covariance matrix: each (isotope, MT, l) triplet gets max_G parameters,
    one for each energy bin, arranged in the same order as param_triplets.
    
    However, we mark which parameters correspond to actual vs padding bins.
    
    Parameters
    ----------
    mf34_cov : MF34CovMat
        MF34 covariance data
        
    Returns
    -------
    Tuple[List[Tuple[int, int, int, int]], Dict[Tuple[int, int, int], List[float]]]
        Parameter mapping with energy bins and energy grids
    """
    # Get unique parameter triplets in the same order as covariance matrix
    param_triplets = mf34_cov._get_param_triplets()
    
    # Create energy grids dictionary
    energy_grids = {}
    
    for i in range(mf34_cov.num_matrices):
        # Add row parameters
        row_param = (mf34_cov.isotope_rows[i], mf34_cov.reaction_rows[i], mf34_cov.l_rows[i])
        energy_grids[row_param] = mf34_cov.energy_grids[i]
        
        # Add column parameters if different
        col_param = (mf34_cov.isotope_cols[i], mf34_cov.reaction_cols[i], mf34_cov.l_cols[i])
        if col_param != row_param:
            energy_grids[col_param] = mf34_cov.energy_grids[i]
    
    # Find maximum energy grid size across all matrices
    max_G = max(matrix.shape[0] for matrix in mf34_cov.matrices) if mf34_cov.matrices else 0
    
    # Create parameter mapping that matches covariance matrix structure exactly
    param_mapping = []
    
    for triplet in param_triplets:
        isotope, mt, l = triplet
        # Add max_G parameters for this triplet, one for each energy bin
        for energy_bin in range(max_G):
            param_mapping.append((isotope, mt, l, energy_bin))
    
    if _get_logger():
        _get_logger().info(f"Created parameter mapping with {len(param_mapping)} parameters (including energy bins)")
        _get_logger().info(f"Parameter triplets: {len(param_triplets)} unique (isotope, MT, l) combinations")
        _get_logger().info(f"Max energy groups: {max_G}")
        _get_logger().info(f"Total parameters: {len(param_triplets)} × {max_G} = {len(param_mapping)}")
        if param_mapping:
            _get_logger().info(f"Sample parameters: {param_mapping[:3]}{'...' if len(param_mapping) > 3 else ''}")
    
    return param_mapping, energy_grids


def _write_sample_summary(
    sample: np.ndarray,
    sample_index: int,
    energy_grids: Dict[Tuple[int, int, int], List[float]],
    param_mapping: List[Tuple[int, int, int, int]],
    sample_dir: str,
    base: str,
):
    """
    Write a summary file for a single sample.
    
    Parameters
    ----------
    sample : np.ndarray
        Perturbation factors for this sample
    sample_index : int
        Sample index
    energy_grids : Dict[Tuple[int, int, int], List[float]]
        Energy grids for each parameter
    param_mapping : List[Tuple[int, int, int, int]]
        Parameter mapping with energy bins
    sample_dir : str
        Sample output directory
    base : str
        Base filename
    """
    sample_str = f"{sample_index+1:04d}"
    summary_file = os.path.join(sample_dir, f"{base}_{sample_str}_summary.txt")
    
    with open(summary_file, 'w') as f:        
        for i, factor in enumerate(sample):
            if i < len(param_mapping):
                isotope, mt, l_coeff, energy_bin = param_mapping[i]
                
                # Get energy bounds for this parameter
                triplet = (isotope, mt, l_coeff)
                energy_grid = energy_grids.get(triplet, [])
                
                # Skip padding bins - only write actual energy bins
                if energy_bin < len(energy_grid) - 1:
                    energy_low = energy_grid[energy_bin]
                    energy_high = energy_grid[energy_bin + 1]
                    
                    # Write with proper alignment for negative numbers
                    # Format: MT(6) L_coeff(6) Energy_Low(15) Energy_High(15) Factor(15)
                    f.write(f"{mt:6d}{l_coeff:6d}{energy_low:15.6e}{energy_high:15.6e}{factor:15.6e}\n")


def _write_master_perturbation_file(
    all_factors: List[np.ndarray],
    all_param_mappings: List[List[Tuple[int, int, int, int]]],
    all_energy_grids: List[Dict[Tuple[int, int, int], List[float]]],
    output_dir: str,
    timestamp: str,
    verbose: bool = True
):
    """
    Write master perturbation factors file in Parquet format.
    
    The format matches ACE perturbation: each row is a sample, each column is a parameter.
    Column names follow the format: Fe56_MT2_L1_E0-E1
    
    Parameters
    ----------
    all_factors : List[np.ndarray]
        List of factor arrays for each ENDF file
    all_param_mappings : List[List[Tuple[int, int, int, int]]]
        List of parameter mappings for each ENDF file with energy bins
    all_energy_grids : List[Dict[Tuple[int, int, int], List[float]]]
        List of energy grids for each ENDF file
    output_dir : str
        Output directory
    timestamp : str
        Timestamp for filename
    verbose : bool
        Whether to log progress
    """
    if not all_factors:
        if verbose and _get_logger():
            _get_logger().info("[ENDF] [OUTPUT] No factors to write to master file")
        return
    
    # Determine the maximum number of samples across all files
    max_samples = max(factors.shape[0] for factors in all_factors)
    
    # Initialize the master DataFrame with Sample_ID column
    master_data = {'Sample_ID': np.arange(1, max_samples + 1, dtype='int32')}
    
    # Process each ENDF file
    for file_idx, (factors, param_mapping, energy_grids) in enumerate(zip(all_factors, all_param_mappings, all_energy_grids)):
        # Convert factors to float32 for consistency
        factors = factors.astype(np.float32)
        
        # Create columns for each parameter in this file
        for param_idx, (isotope, mt, l_coeff, energy_bin) in enumerate(param_mapping):
            # Get element symbol from ZAID
            symbol = zaid_to_symbol(isotope)
            
            # Column name format: Fe56_MT2_L1_E0-E1
            col_name = f"{symbol}_MT{mt}_L{l_coeff}_E{energy_bin}-E{energy_bin+1}"
            
            # Create column data, padding with NaN if this file has fewer samples
            column_data = np.full(max_samples, np.nan, dtype=np.float32)
            if factors.shape[0] > 0:
                column_data[:factors.shape[0]] = factors[:, param_idx]
            
            master_data[col_name] = column_data
    
    # Create DataFrame and save as Parquet
    df = pd.DataFrame(master_data)
    master_file = os.path.join(output_dir, f'endf_perturbation_factors_{timestamp}.parquet')
    df.to_parquet(master_file, index=False, compression='zstd', compression_level=3)
    
    if verbose and _get_logger():
        _get_logger().info(f"[ENDF] [OUTPUT] Master perturbation factors saved to: {master_file}")
        _get_logger().info(f"[ENDF] [OUTPUT] DataFrame shape: {df.shape}")
        _get_logger().info(f"[ENDF] [OUTPUT] Columns: {len(df.columns)} (including Sample_ID)")
        _get_logger().info(f"[ENDF] [OUTPUT] Samples: {max_samples}")
    
    return master_file
